{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notebook used in dataset overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Callable, Any, Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from neural.common.data.datasets import DatasetGenerator\n",
    "from utils.database import DatabaseConnector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_path = Path('images/datasets')\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "sns.set_theme()\n",
    "sns.set_palette('muted')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def all_splits(func: Callable[[str, Any], None]) -> Callable:\n",
    "    def wrapper(*args: Any) -> None:\n",
    "        for split in ['train', 'validation', 'test']:\n",
    "            func(split, *args)\n",
    "\n",
    "    return wrapper"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_entities_examples(dataset_name: str, max_tags: int = 20, skip_sentences: int = 0, entity_type: str = None):\n",
    "    tags_dict = DatabaseConnector().get_tags_dict(dataset_name)\n",
    "    dataset = DatasetGenerator.generate_dataset(dataset_name, 'train')\n",
    "    tags_count = 0\n",
    "    for i, (sentence, tags) in enumerate(dataset):\n",
    "        if i < skip_sentences:\n",
    "            continue\n",
    "        for j, tag in enumerate(tags):\n",
    "            if tag in tags_dict and (entity_type is None or tags_dict[tag][1] == entity_type):\n",
    "                print(sentence[j], tags_dict[tag])\n",
    "                tags_count += 1\n",
    "\n",
    "        if tags_count >= max_tags:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@all_splits\n",
    "def count_dataset_entities(split: str, dataset_name: str) -> None:\n",
    "    tags_dict = DatabaseConnector().get_tags_dict(dataset_name)\n",
    "    all_tags = []\n",
    "    tag_started = None\n",
    "    for _, tags in DatasetGenerator.generate_dataset(dataset_name, split):\n",
    "        for tag in tags:\n",
    "            if tag not in tags_dict:\n",
    "                tag_started = None\n",
    "                continue\n",
    "\n",
    "            tag, category = tags_dict[tag]\n",
    "            position, tag = tag.split('-')\n",
    "            if position == 'I' and tag == tag_started:\n",
    "                continue\n",
    "\n",
    "            if position == 'B':\n",
    "                tag_started = tag\n",
    "\n",
    "            all_tags.append(category)\n",
    "\n",
    "    counter = Counter(all_tags)\n",
    "    print(f'Named entities counts for dataset \"{dataset_name}\" with {split} split:')\n",
    "    for tag, count in counter.most_common():\n",
    "        print(f'{tag}: {count}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@all_splits\n",
    "def print_ner_dataset_distribution(split: str) -> None:\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    article_count = 0\n",
    "    sentence_count = 0\n",
    "    tokens_count = 0\n",
    "    for tokens, tags in DatasetGenerator.generate_dataset('gmb', split):\n",
    "        article_count += 1\n",
    "        sentence_count += len(sent_tokenize(detokenizer.detokenize(tokens)))\n",
    "        tokens_count += len(tokens)\n",
    "    print(f'Dataset stats for {split} split:')\n",
    "    print('Article count:', article_count)\n",
    "    print('Sentence count:', sentence_count)\n",
    "    print('Tokens count:', tokens_count)\n",
    "\n",
    "\n",
    "@all_splits\n",
    "def print_ner_dataset_average_sample_length(split: str, dataset_name: str) -> None:\n",
    "    samples = []\n",
    "    for tokens, _ in DatasetGenerator.generate_dataset(dataset_name, split):\n",
    "        samples.append(len(tokens))\n",
    "\n",
    "    average_length = round(sum(samples) / len(samples))\n",
    "    print(f'Average sample length for {split} split of dataset \"{dataset_name}\": {average_length}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@all_splits\n",
    "def print_summarization_dataset_distribution(split: str, dataset_name: str) -> None:\n",
    "    pairs = 0\n",
    "    article_sentence_count = 0\n",
    "    article_lengths = []\n",
    "    summary_sentence_count = 0\n",
    "    summary_lengths = []\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    for article, summary in DatasetGenerator.generate_dataset(dataset_name, split):\n",
    "        pairs += 1\n",
    "        article_sentence_count += len(sent_tokenize(detokenizer.detokenize(article)))\n",
    "        summary_sentence_count += len(sent_tokenize(detokenizer.detokenize(summary)))\n",
    "        article_lengths.append(len(article))\n",
    "        summary_lengths.append(len(summary))\n",
    "\n",
    "    average_article_length = round(sum(article_lengths) / len(article_lengths))\n",
    "    average_summary_length = round(sum(summary_lengths) / len(summary_lengths))\n",
    "\n",
    "    print(f'Dataset {dataset_name} stats for {split} split:')\n",
    "    print('Pair count:', pairs)\n",
    "    print('Articles sentence count:', article_sentence_count)\n",
    "    print('Articles tokens count:', sum(article_lengths))\n",
    "    print('Average article length:', average_article_length)\n",
    "    print('Summaries sentence count:', summary_sentence_count)\n",
    "    print('Summaries tokens count:', sum(summary_lengths))\n",
    "    print('Average summary length:', average_summary_length)\n",
    "\n",
    "\n",
    "@all_splits\n",
    "def print_summarization_novel_ngrams(split: str, dataset_name: str, n_grams: Sequence[int] = (1, 2, 3, 4)) -> None:\n",
    "    ratios = defaultdict(list)\n",
    "    for article, summary in DatasetGenerator.generate_dataset(dataset_name, split):\n",
    "        article = [token.lower() for token in article]\n",
    "        summary = [token.lower() for token in summary]\n",
    "        for n_gram in n_grams:\n",
    "            article_n_gram = {sequence for sequence in ngrams(article, n_gram)}\n",
    "            summary_n_gram = {sequence for sequence in ngrams(summary, n_gram)}\n",
    "            if len(summary_n_gram) == 0:\n",
    "                continue\n",
    "            novel_n_gram = summary_n_gram - article_n_gram\n",
    "            novel_ratio = len(novel_n_gram) / len(summary_n_gram)\n",
    "            ratios[n_gram].append(novel_ratio)\n",
    "\n",
    "    print(f'Novel n-grams in {split} split of \"{dataset_name}\" dataset:')\n",
    "    for n_gram, ratio in ratios.items():\n",
    "        ratio = sum(ratio) / len(ratio)\n",
    "        ratio = round(ratio * 100, 2)\n",
    "        print(f'Novel {n_gram}-gram ratio: {ratio}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_ner_dataset_lengths(splits: Sequence[str] = ('train', 'validation', 'test')) -> None:\n",
    "    conll_tokens_lengths = []\n",
    "    gmb_tokens_lengths = []\n",
    "    for split in splits:\n",
    "        for tokens, tags in DatasetGenerator.generate_dataset('conll2003', split):\n",
    "            conll_tokens_lengths.append(len(tokens))\n",
    "\n",
    "        for tokens, tags in DatasetGenerator.generate_dataset('gmb', split):\n",
    "            gmb_tokens_lengths.append(len(tokens))\n",
    "\n",
    "    data = {\n",
    "        'CoNLL-2003': conll_tokens_lengths,\n",
    "        'GMB': gmb_tokens_lengths\n",
    "    }\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    plt.xlim(0, 170)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Data sample length', fontsize=14)\n",
    "    plt.ylabel('Number of samples', fontsize=14)\n",
    "    plot = sns.histplot(data, binwidth=2)\n",
    "    plt.setp(plot.get_legend().get_texts(), fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(save_path / f'ner_datasets_lengths.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_ner_examples(dataset_name: str, examples_number: int = 3, skip_examples: int = 0) -> None:\n",
    "    tags_dict = DatabaseConnector().get_tags_dict(dataset_name)\n",
    "    for i, (tokens, tags) in enumerate(DatasetGenerator.generate_dataset(dataset_name, 'train')):\n",
    "        if i < skip_examples:\n",
    "            continue\n",
    "        if i == examples_number + skip_examples:\n",
    "            break\n",
    "\n",
    "        tags = [tags_dict[tag][0] if tag in tags_dict else 'O' for tag in tags]\n",
    "        tokens_str = ' '.join(tokens)\n",
    "        tags_str = ' '.join(tags)\n",
    "        print(tokens_str)\n",
    "        print(tags_str)\n",
    "        print()\n",
    "\n",
    "\n",
    "def plot_summaries_sizes_single_ax(ax: plt.Axes, data: Dict[str, List[int]], name: str, x_lim: int,\n",
    "                                   bin_width: int = 2) -> None:\n",
    "    ax.set_xlim(0, x_lim)\n",
    "    ax.set_xlabel(f'{name.capitalize()} lengths', fontsize=26)\n",
    "    ax.set_ylabel(f'Number of {name}', fontsize=26)\n",
    "    ax.set_title(f'{name.capitalize()} length comparison.', fontsize=30)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=20)\n",
    "    plot = sns.histplot(data, binwidth=bin_width, ax=ax)\n",
    "    plt.setp(plot.get_legend().get_texts(), fontsize=26)\n",
    "\n",
    "\n",
    "def plot_summaries_dataset_lengths(splits: Sequence[str] = ('train', 'validation', 'test')) -> None:\n",
    "    cnn_articles_len = []\n",
    "    cnn_summaries_len = []\n",
    "    xsum_articles_len = []\n",
    "    xsum_summaries_len = []\n",
    "\n",
    "    for split in splits:\n",
    "        for article, summary in DatasetGenerator.generate_dataset('cnn_dailymail', split):\n",
    "            cnn_articles_len.append(len(article))\n",
    "            cnn_summaries_len.append(len(summary))\n",
    "\n",
    "        for article, summary in DatasetGenerator.generate_dataset('xsum', split):\n",
    "            xsum_articles_len.append(len(article))\n",
    "            xsum_summaries_len.append(len(summary))\n",
    "\n",
    "    article_data = {\n",
    "        'CNN/Daily Mail': cnn_articles_len,\n",
    "        'XSum': xsum_articles_len\n",
    "    }\n",
    "\n",
    "    summary_data = {\n",
    "        'CNN/Daily Mail': cnn_summaries_len,\n",
    "        'XSum': xsum_summaries_len\n",
    "    }\n",
    "\n",
    "    fig, (article_ax, summary_ax) = plt.subplots(2, 1, figsize=(25, 16))\n",
    "    plot_summaries_sizes_single_ax(article_ax, article_data, 'articles', x_lim=2000, bin_width=20)\n",
    "    plot_summaries_sizes_single_ax(summary_ax, summary_data, 'summaries', x_lim=150)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(save_path / f'summarization_datasets_lengths.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_summarization_examples(dataset_name: str, examples_number: int = 3, skip_examples: int = 0) -> None:\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    for i, (article, summary) in enumerate(DatasetGenerator.generate_dataset(dataset_name, 'train')):\n",
    "        if i < skip_examples:\n",
    "            continue\n",
    "        if i == examples_number + skip_examples:\n",
    "            break\n",
    "\n",
    "        print(detokenizer.detokenize(article))\n",
    "        print(15 * '-')\n",
    "        print(detokenizer.detokenize(summary))\n",
    "        print(50 * '-')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_entities_examples('conll2003')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_entities_examples('gmb')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_ner_dataset_distribution()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_ner_dataset_average_sample_length('conll2003')\n",
    "print_ner_dataset_average_sample_length('gmb')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_summarization_dataset_distribution('cnn_dailymail')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_summarization_dataset_distribution('xsum')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_ner_dataset_lengths()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count_dataset_entities('conll2003')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count_dataset_entities('gmb')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_summaries_dataset_lengths()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_ner_examples('conll2003')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_ner_examples('gmb')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_summarization_examples('cnn_dailymail', examples_number=15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_summarization_examples('xsum', examples_number=15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_summarization_novel_ngrams('cnn_dailymail')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_summarization_novel_ngrams('xsum')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}