{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook is used for generating examples mentioned in my thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path_to_images = Path('./images')\n",
    "path_to_images.mkdir(parents=True, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"We're going shopping today, but yesterday we went to the \"\n",
    "\"cinema because shops were closed.\"\n",
    "tokens = word_tokenize(sentence.lower())\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stem_tokens = [stemmer.stem(token) for token in tokens]\n",
    "print(stem_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bag_of_words = Counter(tokens)\n",
    "print(bag_of_words)\n",
    "bag_of_words_stem = Counter(stem_tokens)\n",
    "print(bag_of_words_stem)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vocab\n",
    "\n",
    "vocab = Vocab(bag_of_words, max_size=15, specials=['<unk>'])\n",
    "word_vector = [vocab.stoi[token] for token in tokens]\n",
    "print(word_vector)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "glove = gensim.downloader.load('glove-wiki-gigaword-50')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_words = ['king', 'science', 'computer']\n",
    "vectors = []\n",
    "labels = []\n",
    "for sample_word in sample_words:\n",
    "    vectors.append(glove[sample_word])\n",
    "    labels.append(sample_word)\n",
    "    similar_words = glove.most_similar(sample_word, topn=10)\n",
    "    for word, _ in similar_words:\n",
    "        vectors.append(glove[word])\n",
    "        labels.append(word)\n",
    "\n",
    "vectors = np.asarray(vectors)\n",
    "vectors.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_vectors = pca.fit_transform(vectors)\n",
    "pca_vectors.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(pca_vectors[:, 0], pca_vectors[:, 1])\n",
    "annotations = []\n",
    "for i, label in enumerate(labels):\n",
    "    annotations.append(plt.annotate(label, (pca_vectors[i, 0], pca_vectors[i, 1])))\n",
    "adjust_text(annotations)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = nlp(\n",
    "    \"The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep. Germany's representative to the European Union's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer. He said further scientific study was required and if it was found that action was needed it should be taken by the European Union.\"\n",
    ")\n",
    "displacy.render(text, style='ent', jupyter=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from rouge_score.rouge_scorer import RougeScorer\n",
    "\n",
    "rouge = RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "prediction = 'Today in the zoo we are. Tomorrow cinema.'\n",
    "target = 'We are going to the zoo today, but tomorrow we will go the cinema.'\n",
    "scores = rouge.score(target, prediction)\n",
    "for score, values in scores.items():\n",
    "    print(score, round(values.fmeasure, 4), end=' ')\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "np.random.seed(2)\n",
    "points_x = np.arange(30)\n",
    "points_y = np.random.randn(30)\n",
    "train_x, test_x, train_y, test_y = train_test_split(points_x, points_y, train_size=0.7)\n",
    "train_x.sort()\n",
    "\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "\n",
    "plt.scatter(train_x, train_y, c='royalblue', s=150)\n",
    "plt.scatter(test_x, test_y, c='indianred', s=150)\n",
    "\n",
    "x_plot = np.linspace(train_x.min(), train_x.max(), 300)\n",
    "spline = make_interp_spline(train_x, train_y, k=3)  # type: BSpline\n",
    "y_plot = spline(x_plot)\n",
    "plt.plot(x_plot, spline(x_plot), lw=2)\n",
    "\n",
    "plt.savefig('images/overfitting.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}