{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "from collections import defaultdict\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import neural.common.utils as utils\n",
    "from neural.common.data.vocab import Vocab, VocabBuilder\n",
    "from neural.common.scores import ROUGE\n",
    "from neural.summarization.dataloader import SummarizationDataset, SummarizationDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_path = Path('../data/saved/models')\n",
    "device = utils.get_device(use_cuda=True)\n",
    "utils.set_random_seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_trained_model(model_id: str, path_to_model: Path, weights_name: str = None, **params: Any) -> nn.Module:\n",
    "    module = import_module(f'neural.train_{model_id}')\n",
    "    args = utils.load_args_from_file(path_to_model)\n",
    "    model_name = path_to_model.stem\n",
    "    weights_path = path_to_model / f'{model_name}.pt'\n",
    "\n",
    "    model = module.create_model_from_args(args, **params)\n",
    "    weights = torch.load(weights_path, map_location=device)\n",
    "    weights_name = weights_name or model_name\n",
    "    model.load_state_dict(weights[f'{weights_name}_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    del weights\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def count_model_parameters(model: nn.Module) -> None:\n",
    "    learnable_parameters = defaultdict(int)\n",
    "    constant_parameters = defaultdict(int)\n",
    "    model.train()\n",
    "    for name, parameter in model.named_parameters():\n",
    "        parameter_name = name.split('.weight')[0].split('.bias')[0]\n",
    "        if parameter.requires_grad:\n",
    "            learnable_parameters[parameter_name] += torch.numel(parameter)\n",
    "        else:\n",
    "            constant_parameters[parameter_name] += torch.numel(parameter)\n",
    "    model.eval()\n",
    "\n",
    "    if len(learnable_parameters) > 0:\n",
    "        print('Learnable parameters:')\n",
    "        for name, count in learnable_parameters.items():\n",
    "            print(f'{name}: {count}')\n",
    "        print(f'Sum: {sum(learnable_parameters.values())}')\n",
    "\n",
    "    if len(constant_parameters) > 0:\n",
    "        print('Constant parameters:')\n",
    "        for name, count in constant_parameters.items():\n",
    "            print(f'{name}: {count}')\n",
    "        print(f'Sum: {sum(constant_parameters.values())}')\n",
    "\n",
    "\n",
    "def show_examples_summarization(model: nn.Module, loader: SummarizationDataLoader, vocab: Vocab,\n",
    "                                predict_tokens: Callable[[nn.Module, Tuple[Any, ...]], Tensor],\n",
    "                                use_oov: bool = True, examples_number: int = 3) -> None:\n",
    "    scorer = ROUGE(vocab, 'rouge1')\n",
    "    examples = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(loader):\n",
    "            inputs = utils.convert_input_to_device(inputs, device)\n",
    "            if use_oov:\n",
    "                targets, oov_list = inputs[-2:]\n",
    "            else:\n",
    "                oov_list = None\n",
    "                targets = inputs[-1]\n",
    "\n",
    "            tokens = predict_tokens(model, inputs)\n",
    "            for i in range(tokens.shape[1]):\n",
    "                if use_oov:\n",
    "                    utils.add_words_to_vocab(vocab, oov_list[i])\n",
    "\n",
    "                score_out = tokens[:, i].unsqueeze(dim=1)\n",
    "                score_target = targets[:, i].unsqueeze(dim=1)\n",
    "                score = scorer.score(score_out, score_target)\n",
    "\n",
    "                score_out = utils.clean_predicted_tokens(score_out, 3)\n",
    "                score_out = utils.remove_unnecessary_padding(score_out)\n",
    "                score_out = utils.tensor_to_string(vocab, score_out)\n",
    "                score_target = utils.remove_unnecessary_padding(score_target)\n",
    "                score_target = utils.tensor_to_string(vocab, score_target)\n",
    "\n",
    "                if use_oov:\n",
    "                    utils.remove_words_from_vocab(vocab, oov_list[i])\n",
    "                heapq.heappush(examples, (score['ROUGE-1'], (score_out, score_target)))\n",
    "\n",
    "    print_examples(examples, examples_number, best=True)\n",
    "    print_examples(examples, examples_number, best=False)\n",
    "\n",
    "\n",
    "def print_examples(examples: List[Tuple[float, Tuple[str, str]]], examples_number: int, best: bool) -> None:\n",
    "    if best:\n",
    "        examples_type = 'Best'\n",
    "        examples_generator = heapq.nlargest\n",
    "    else:\n",
    "        examples_type = 'Worst'\n",
    "        examples_generator = heapq.nsmallest\n",
    "\n",
    "    print(f'{examples_type} examples:')\n",
    "    for score, (prediction, target) in examples_generator(examples_number, examples):\n",
    "        print('ROUGE-1: ', score)\n",
    "        print('Prediction:', prediction)\n",
    "        print()\n",
    "        print('Target:', target)\n",
    "        print(50 * '-')\n",
    "\n",
    "\n",
    "def predict_pointer_generator(model: nn.Module, inputs: Tuple[Any, ...]) -> Tensor:\n",
    "    texts, texts_lengths, summaries, summaries_lengths, texts_extended, targets, oov_list = inputs\n",
    "    oov_size = len(max(oov_list, key=lambda x: len(x)))\n",
    "    _, tokens, _, _ = model(texts, texts_lengths, texts_extended, oov_size)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def predict_rl(model: nn.Module, inputs: Tuple[Any, ...]) -> Tensor:\n",
    "    texts, texts_lengths, summaries, summaries_lengths, texts_extended, targets, oov_list = inputs\n",
    "    oov_size = len(max(oov_list, key=lambda x: len(x)))\n",
    "    _, tokens, _ = model(texts, texts_lengths, texts_extended, oov_size)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def predict_transformer(model: nn.Module, inputs: Tuple[Any, ...]) -> Tensor:\n",
    "    texts, _, summaries, _, targets = inputs\n",
    "    _, tokens = model(texts)\n",
    "\n",
    "    return tokens\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pointer_generator = load_trained_model('pointer_generator', models_path / 'pointer_generator', bos_index=2, eos_index=3,\n",
    "                                       unk_index=1)\n",
    "pointer_generator.activate_coverage()\n",
    "count_model_parameters(pointer_generator)\n",
    "vocab = VocabBuilder.build_vocab('cnn_dailymail', 'summarization', vocab_size=50000)\n",
    "dataset = SummarizationDataset('cnn_dailymail', 'test', 400, 100, vocab, get_oov=True)\n",
    "dataloader = SummarizationDataLoader(dataset, batch_size=32)\n",
    "show_examples_summarization(pointer_generator, dataloader, vocab, predict_pointer_generator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reinforcement_summarization = load_trained_model('rl', models_path / 'reinforcement_learning', weights_name='rl_model',\n",
    "                                                 bos_index=2, eos_index=3, unk_index=1)\n",
    "count_model_parameters(reinforcement_summarization)\n",
    "vocab = VocabBuilder.build_vocab('cnn_dailymail', 'summarization', vocab_size=50000)\n",
    "dataset = SummarizationDataset('cnn_dailymail', 'test', 800, 100, vocab, get_oov=True)\n",
    "dataloader = SummarizationDataLoader(dataset, batch_size=32)\n",
    "show_examples_summarization(reinforcement_summarization, dataloader, vocab, predict_rl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transformer = load_trained_model('transformer', models_path / 'transformer', bos_index=2, eos_index=3)\n",
    "count_model_parameters(transformer)\n",
    "vocab = VocabBuilder.build_vocab('cnn_dailymail', 'summarization', vocab_size=50000)\n",
    "dataset = SummarizationDataset('cnn_dailymail', 'test', 400, 100, vocab)\n",
    "dataloader = SummarizationDataLoader(dataset, batch_size=16)\n",
    "show_examples_summarization(transformer, dataloader, vocab, predict_transformer, use_oov=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}